{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9966aca0-399c-4878-9e0d-4b11549da08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CUDA_LAUNCH_BLOCKING=1 python BERT+CRF.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed3eb181-c030-48a7-aef2-5a79c6a8b443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchcrf\n",
      "  Downloading TorchCRF-1.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.11/site-packages (from torchcrf) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torchcrf) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->torchcrf) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchcrf) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\n",
      "Downloading TorchCRF-1.1.0-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: torchcrf\n",
      "Successfully installed torchcrf-1.1.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: seqeval in ./.local/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.local/lib/python3.11/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.5.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.15.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.local/lib/python3.11/site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in ./.local/lib/python3.11/site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./.local/lib/python3.11/site-packages (from evaluate) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in ./.local/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.local/lib/python3.11/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./.local/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.local/lib/python3.11/site-packages (from evaluate) (0.29.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchcrf\n",
    "!pip install transformers datasets seqeval\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bbf46d-9895-4ea2-91e6-1b6c3089e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 12:02:09.734993: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741521729.747460    2621 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741521729.751210    2621 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from TorchCRF import CRF\n",
    "from transformers import AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import numpy as np\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80770c37-8e99-4386-874f-a24064f57527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ab8cff5-22e3-469e-a2fd-f3acd9185c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataset(json_path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file with token and label data, flattens it,\n",
    "    converts labels to numeric IDs, and splits the data into train/test sets.\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict: A Hugging Face DatasetDict with 'train' and 'test' splits.\n",
    "    \"\"\"\n",
    "    js = pd.read_json(json_path, encoding=\"utf-8\")\n",
    "    tokens_flat = []\n",
    "    labels_flat = []\n",
    "    sentence_ids_flat = []\n",
    "    \n",
    "    # Flatten the lists\n",
    "    for _, row in js.iterrows():\n",
    "        tokens = row[\"tokens\"]\n",
    "        labels = row[\"labels\"]\n",
    "        sentence_id = row[\"sentence_id\"]  # This is an integer\n",
    "    \n",
    "        # Ensure token-label length matches before extending\n",
    "        if len(tokens) == len(labels):\n",
    "            tokens_flat.extend(tokens)  \n",
    "            labels_flat.extend(labels)  \n",
    "            sentence_ids_flat.extend([sentence_id] * len(tokens))  # Repeat sentence_id for each token\n",
    "        else:\n",
    "            print(f\"Skipping sentence_id {sentence_id} due to mismatched lengths: {len(tokens)} tokens vs {len(labels)} labels\")\n",
    "    \n",
    "    # Ensure final lengths match\n",
    "    assert len(tokens_flat) == len(labels_flat) == len(sentence_ids_flat), \"Mismatch in list lengths!\"\n",
    "    \n",
    "    # Convert labels to numeric IDs\n",
    "    unique_labels = list(set(labels_flat))\n",
    "\n",
    "    global id2label\n",
    "    id2label = {idx: label for idx, label in enumerate(unique_labels)}     \n",
    "    global label2id\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    labels_numeric_flat = [label2id[label] for label in labels_flat]\n",
    "    \n",
    "    # Create Dataset dictionary\n",
    "    dataset_dict = {\n",
    "        \"tokens\": tokens_flat,\n",
    "        \"ner_tags\": labels_flat,\n",
    "        \"sentence_id\": sentence_ids_flat,\n",
    "        \"labels_numeric\": labels_numeric_flat\n",
    "    }\n",
    "    \n",
    "    # Convert to Hugging Face Dataset\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    # Store in DatasetDict format\n",
    "    dataset_final = DatasetDict({\n",
    "        \"train\": dataset_split[\"train\"],\n",
    "        \"test\": dataset_split[\"test\"]\n",
    "    })\n",
    "    \n",
    "    # Display a sample\n",
    "    print(dataset_final[\"train\"][0])\n",
    "    print(f\"Training set size: {len(dataset_final['train'])}\")\n",
    "    print(f\"Test set size: {len(dataset_final['test'])}\")\n",
    "\n",
    "    return dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55324b87-3424-4f5f-abf9-fedce57942e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2ab89c092f4cccbf01bbb734b831c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b65973ae074c2aa94068b799792d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64660562f95c49d3bfc612b101cf9831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': 'to', 'ner_tags': 'O', 'sentence_id': 1, 'labels_numeric': 6}\n",
      "Training set size: 393\n",
      "Test set size: 99\n",
      "Tokens: [None, 0, 1, 2, 3, 4, 4, None]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name= \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "# model_name= \"ProsusAI/finbert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "dataset= create_dataset('Annoted_JSON_090225.json')\n",
    "token = tokenizer(dataset['train']['tokens'][:5], is_split_into_words=True, padding=True, truncation=True)\n",
    "\n",
    "print(f\"Tokens: {token.word_ids()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c84f554-8c74-4ef0-ad2f-5807044c423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class BertCRFNER(nn.Module):\n",
    "#     def __init__(self, bert_model_name, num_labels):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "#         self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    \n",
    "#     def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "#         outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "#         logits = self.classifier(outputs.last_hidden_state)\n",
    "        \n",
    "#         if labels is not None:\n",
    "#             loss_fn = nn.CrossEntropyLoss()\n",
    "#             loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "#             return {\"loss\": loss, \"logits\": logits}  # Ensure loss is returned\n",
    "        \n",
    "#         return {\"logits\": logits}  # No loss during inference\n",
    "\n",
    "# # Load the model\n",
    "# model_name = \"bert-base-cased\"\n",
    "# num_labels = len(set(dataset[\"train\"][\"ner_tags\"]))  \n",
    "# model = BertCRFNER(model_name, num_labels)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e307f9-91cd-4ad0-a3b9-06482d357940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc45b7baec94a99b1ca2395b47c4690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Numeric: [6, 6, 6, 9, 8, 19, 17, 14, 6, 6, 9, 6, 6, 3, 9, 19, 8, 6, 13, 6, 19, 6, 6, 6, 24, 6, 17, 13, 3, 6, 6, 6, 5, 8, 6, 28, 6, 6, 17, 30, 19, 6, 6, 6, 11, 6, 23, 6, 19, 5, 6, 6, 30, 6, 6, 6, 6, 18, 14, 7, 28, 13, 28, 7, 6, 1, 30, 6, 6, 6, 6, 6, 6, 6, 6, 11, 7, 28, 0, 19, 28, 6, 6, 6, 6, 6, 6, 28, 17, 28, 6, 6, 6, 6, 6, 6, 6, 6, 6, 23, 6, 2, 6, 19, 6, 6, 6, 6, 19, 6, 6, 10, 19, 6, 17, 6, 6, 6, 28, 6, 30, 18, 6, 30, 6, 30, 6, 1, 6, 6, 6, 6, 24, 6, 6, 6, 30, 6, 6, 6, 30, 24, 6, 19, 6, 19, 21, 6, 19, 2, 6, 17, 6, 6, 6, 9, 6, 21, 16, 6, 6, 6, 7, 6, 30, 6, 30, 6, 6, 6, 14, 24, 28, 6, 6, 7, 6, 19, 6, 6, 6, 30, 6, 6, 6, 6, 6, 6, 12, 6, 6, 7, 7, 30, 19, 6, 6, 6, 19, 6, 19, 6, 30, 6, 12, 19, 6, 6, 6, 6, 6, 22, 6, 30, 5, 24, 9, 3, 6, 6, 6, 7, 6, 6, 27, 6, 6, 6, 16, 6, 6, 6, 1, 6, 6, 6, 3, 9, 6, 8, 30, 3, 6, 30, 6, 6, 6, 28, 6, 6, 6, 6, 6, 21, 6, 6, 6, 6, 6, 6, 28, 6, 7, 17, 0, 6, 6, 6, 6, 12, 19, 10, 6, 6, 25, 6, 6, 6, 6, 6, 26, 11, 6, 6, 6, 6, 6, 30, 6, 6, 6, 20, 19, 6, 30, 5, 6, 6, 15, 5, 6, 6, 6, 25, 6, 6, 6, 6, 6, 6, 19, 6, 21, 8, 24, 6, 17, 17, 13, 21, 4, 6, 6, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 6, 30, 21, 12, 6, 5, 7, 6, 19, 0, 6, 6, 28, 17, 6, 6, 11, 30, 6, 12, 6, 6, 6, 20, 6, 6, 6, 6, 6, 6, 2, 6, 6, 3, 6, 6, 6, 6, 19, 6, 16, 6, 30, 6, 6, 6, 6, 6, 29, 6, 6, 9, 6, 6, 6, 6, 18, 27, 27, 6]\n",
      "Tokenized Labels: {'input_ids': array([[  101,  1106,  1110,  1264,  1104,  7270,  2069,  3389, 17881,\n",
      "         1527,   117,  1990,  2166,  1447, 10272,  1106,  7516,   836,\n",
      "        17175,   117,  3413,  1559, 10272,  1231, 11031, 17881,  1477,\n",
      "          131,  1447,   139,  5123, 13130,  1403,  1103,  2934,  1106,\n",
      "         2915,   119,  1542,   110,   182,  1495,   119,  1105, 17881,\n",
      "         1495,   119, 24824,   787,   122,   117,  3614,  1475,   119,\n",
      "          130,  1703,  1955,   117,  5128, 13081,   113, 17881,  1477,\n",
      "          131,  1103,   123,   119,  4389,  7516,  4544, 17881,  1527,\n",
      "          117,  8160,  1844,  1108,  5118,  1134,  1550,   114,   119,\n",
      "         1110,  1550,  1104,  3389,  4570, 10558,  2557,   132,  4570,\n",
      "         1157,  6455,  1105,  1106, 17881,  1495,   117, 26768,  1116,\n",
      "         1550,   122,   119,  1765,  1419,   836,  1580,   119,   121,\n",
      "         1550,  7539,   117,   113, 13063,   119, 12398,  1104,  1120,\n",
      "         1990,  1107, 12087,  5542,  7516,   119,  1550,   114,  3775,\n",
      "          119,  5731,  1604,   119,   130,   139,  5123, 13130,  1403,\n",
      "         1447,   125,   119,  5129,  1121,   113, 14243,  1103,  1106,\n",
      "         1107,  3772,   836,  1559,  1545, 17881,  1527,   117,  7967,\n",
      "         1103,  1120,  2798,  1214,   119,  1125,  1110,  1104,  1104,\n",
      "         1120,  1550,  8941,  7270,  2069,  3884,  1447,  3813,   125,\n",
      "          119,   121,  2075,   170, 13588,  1107,  1550,  7270,  2069,\n",
      "         1447,  1108, 17881,  1527,   117,  1142,  1106,  1955,   117,\n",
      "         1743,  1580,   170,  4692, 17881,  1495,   117, 19534,   787,\n",
      "          188,  4529,   119,  1104,  2467, 12087,  1347,  2971,   170,\n",
      "         1104,  3735,  3775,  2773, 12087,  1496, 10602,  1108,  1106,\n",
      "        12087,   142, 24963,  9159,  4865,  1142,  5795,  1111, 19821,\n",
      "          124,   117,  5117,  1571,  1113,  1447,  1550,  3458, 17881,\n",
      "         1527,   117,  1120,  1412,  1214,  2699,  4541, 17881,   117,\n",
      "         5465,  1495,  1550,  1106,  1188, 24930,  4902,  1550,   117,\n",
      "         1130,   142, 24963,  9159,   174,  2497, 18876,  1891,  7143,\n",
      "         5542,   170,  1447,  1990,  4865,   119,  1367,   119,   121,\n",
      "         1107,  1104,  3775,  1120,  4570,  1107,  1907,  1496,  8799,\n",
      "         4147,  1103,  1114,  6357,   119,  1130,  1406,   119,   121,\n",
      "          110,  7270,  2069,  1114,  1103,  3775,   119,  3775,  4692,\n",
      "         4434,  8926,  1126,  1214, 12147,  1107,   155,   111,   141,\n",
      "         1105,  5022,  1130,  7270,  2069,  2578,  4801,  3256, 24106,\n",
      "         1116,  1103,  1329,   119,  1550,  3469,  8160,  4541,  4865,\n",
      "         1105, 18513,   119,   130,  2905,  1146,  1120,  3775,   119,\n",
      "         1112,  1103, 12242,  1189,   127,   110,  9211,  1550,  1137,\n",
      "         1105,  2500,  1382,  1114,   170,   182,  1495,  1367,   119,\n",
      "          129,  2130,  7694, 17881,  1477, 23343,   122,   117, 12737,\n",
      "          119,   123,  1295,  1447,  1955,   117,  1132,  1214,   787,\n",
      "          188,   836,  1545,   119,   124,  3549,  1335,  3945,  2182,\n",
      "         3759,  1367,  2324,  1125,  3884,  6455,  1703,  1338,   121,\n",
      "          119,  5486,  1707,   117,  1550, 17881,  1495,   119,   139,\n",
      "         5123, 13130,  1403, 11279,   119,  1146,  1107,  2211,  7270,\n",
      "         2069, 20961,  7270,  2069,  1107,  1104,   836, 26752,   117,\n",
      "        21338,  1517,   118,  1194,   113,  2166,  1107,  1111,  1249,\n",
      "          182,  1495,   117,  1550,   114,   119,  1447,  1104,  2103,\n",
      "         2934,  1107,  1718,  4311,  1130,  1130,  1489,   119,  1744,\n",
      "        12966,  1107, 21804, 17034,  1107,  1198, 17881,   117,  5465,\n",
      "         1495,   117, 15689,  1106,  1103,  1107, 22690,   117,  5347,\n",
      "         1604,  1109,  1134,  1110,  1103,  1214,   131,  1106,  8653,\n",
      "         2798,  1851,  1571, 17881,  1495,  4865,   117,  4841, 14765,\n",
      "        17881,  1495,   117, 17881,  1527,   117,  9211,   126,   117,\n",
      "         3127,  1550,  1105,  1185,  1703, 24224,  2569,   113, 17881,\n",
      "         1477,   131,  1107,  1104,  7270,  2069,   117,  1108,  2166,\n",
      "         1195, 11928,  2724,  1559,   117,  1288,  7270,   102]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1]]), 'labels': [[-100, 6, 6, 6, 9, 8, 8, 19, 17, 17, 17, 14, 6, 6, 9, 6, 6, 3, 3, 3, 3, 3, 9, 19, 19, 8, 8, 8, 6, 13, 13, 13, 13, 6, 19, 6, 6, 6, 6, 6, 24, 24, 24, 6, 17, 17, 17, 13, 13, 3, 3, 3, 3, 3, 3, 6, 6, 6, 6, 5, 8, 8, 8, 8, 6, 28, 28, 28, 6, 6, 17, 17, 17, 30, 19, 6, 6, 6, 11, 11, 11, 6, 23, 6, 19, 5, 6, 6, 6, 30, 6, 6, 6, 6, 18, 18, 18, 14, 14, 7, 28, 28, 28, 13, 28, 28, 28, 28, 7, 6, 6, 1, 1, 1, 30, 6, 6, 6, 6, 6, 6, 6, 6, 11, 11, 7, 7, 28, 28, 28, 28, 0, 0, 0, 0, 19, 28, 28, 28, 6, 6, 6, 6, 6, 6, 6, 28, 28, 28, 17, 17, 17, 28, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 23, 6, 2, 2, 6, 19, 6, 6, 6, 6, 6, 6, 19, 6, 6, 10, 10, 19, 6, 17, 17, 17, 6, 6, 6, 6, 28, 28, 6, 30, 18, 18, 18, 6, 6, 6, 30, 30, 6, 30, 6, 1, 6, 6, 6, 6, 24, 6, 6, 6, 30, 6, 6, 6, 30, 30, 30, 24, 6, 19, 6, 19, 21, 21, 21, 21, 6, 19, 2, 6, 17, 17, 17, 6, 6, 6, 9, 6, 21, 21, 21, 21, 16, 6, 6, 6, 6, 7, 7, 6, 30, 30, 30, 6, 6, 6, 6, 30, 6, 6, 6, 14, 24, 24, 28, 28, 28, 6, 6, 7, 6, 19, 6, 6, 6, 30, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 12, 12, 6, 6, 7, 7, 7, 30, 19, 6, 6, 6, 19, 6, 19, 19, 19, 6, 30, 6, 12, 12, 19, 6, 6, 6, 6, 6, 6, 6, 22, 6, 30, 5, 24, 9, 3, 3, 3, 6, 6, 6, 7, 7, 6, 6, 27, 6, 6, 6, 6, 16, 6, 6, 6, 1, 6, 6, 6, 6, 3, 3, 3, 9, 6, 8, 8, 30, 3, 3, 3, 3, 3, 6, 30, 6, 6, 6, 6, 6, 6, 28, 28, 28, 28, 6, 6, 6, 6, 6, 21, 6, 6, 6, 6, 6, 6, 28, 28, 28, 6, 6, 7, 17, 17, 17, 0, 0, 0, 0, 6, 6, 6, 6, 6, 12, 12, 19, 10, 10, 6, 6, 25, 25, 25, 25, 6, 6, 6, 6, 6, 6, 6, 6, 26, 26, 26, 11, 11, 11, 6, 6, 6, 6, 6, 30, 6, 6, 6, 20, 20, 20, 19, 6, 30, 5, 6, 6, 15, 15, 15, 15, 15, 5, 6, 6, 6, 25, 25, 25, 25, 6, 6, 6, 6, 6, 6, 6, 19, 6, 21, 21, 8, 8, 24, 24, 6, 6, 17, 17, 17, 17, 17, 17, 13, 21, 21, 21, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 6, 6, 8, 8, 6, 6, 6, 6, 30, 21, 21, 21, 21, 12, -100]]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80abe854d3c416b93196004028ad006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Numeric: [20, 19, 13, 6, 6, 6, 6, 16, 6, 6, 6, 6, 6, 30, 6, 11, 6, 6, 13, 6, 20, 15, 28, 6, 6, 6, 1, 6, 6, 6, 6, 6, 10, 16, 6, 6, 6, 28, 10, 30, 6, 6, 6, 15, 6, 28, 6, 19, 6, 6, 6, 6, 6, 6, 6, 6, 25, 7, 6, 6, 6, 5, 6, 6, 6, 6, 30, 6, 20, 6, 6, 6, 12, 30, 6, 19, 27, 1, 6, 6, 6, 6, 6, 8, 6, 19, 6, 6, 7, 6, 8, 6, 6, 6, 2, 21, 20, 6, 28]\n",
      "Tokenized Labels: {'input_ids': array([[  101,   122,   117,  7393,   119,   126,  1703, 24824,  1105,\n",
      "         2075,  1104,  1977,  1550,  1214,  1214,   119,  1110,  1106,\n",
      "         1634,  7991,  1104,  1550,   114,   117,  2426, 12087, 24824,\n",
      "         1109, 14691,   119,   122,  1542,   117,  3102,  1580,  1479,\n",
      "          119,  5787,  1703,  2500,  1110,  1382,  5671,   114,  1118,\n",
      "         1109,  1114,  3254, 17482,  5382,  7270,  2069,  1550,  1103,\n",
      "         1108,  1447,   836,  1475,   117,  5599,  1604,  7270,  2069,\n",
      "         1447,  1338,  1103,  1353,   117,   836, 22639,   117, 21606,\n",
      "         1205,  4062,   117,  4372,  1104,  4434,  1107,  1355,  2039,\n",
      "         1107,  1412, 12087,  5942,  1571,  2058,  1542,   117, 22120,\n",
      "          114,   119,  3775,  3968,  2500,  1195,  3813,  1104,  4376,\n",
      "         1476,   117,  1103,  5671,  1118, 13414,   119,   126,   170,\n",
      "         1105,  2482,   119,  7270,  2069,  6053,  1107,  4570, 12242,\n",
      "         1347,  1214,   119, 12087,  1249,  9045,  6298,   113, 17881,\n",
      "         1477,   131,  1104, 20961,  1105,  1106,  1550,  1105,   113,\n",
      "        17881,  1477,   131,  1103,  1104,  1121,  1550,  5615,   110,\n",
      "          119,  1542,   117,  5306,  1604,  1103,  1743,   110,   102]]), 'token_type_ids': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': [[-100, 20, 20, 20, 20, 20, 19, 13, 6, 6, 6, 6, 16, 6, 6, 6, 6, 6, 6, 30, 6, 11, 11, 11, 6, 6, 13, 6, 20, 20, 20, 15, 15, 15, 15, 28, 28, 28, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 10, 10, 16, 6, 6, 6, 28, 28, 28, 28, 28, 10, 10, 30, 6, 6, 6, 6, 15, 15, 15, 15, 6, 28, 28, 28, 6, 19, 6, 6, 6, 6, 6, 6, 6, 6, 6, 25, 25, 25, 25, 25, 7, 6, 6, 6, 5, 6, 6, 6, 6, 6, 30, 6, 20, 20, 20, 6, 6, 6, 6, 12, 12, 30, 6, 19, 27, 1, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 6, 19, 6, 6, 7, 6, 8, 8, 8, 8, 6, 6, 6, 2, 21, 21, 21, 20, 20, 20, 20, 6, 28, 28, -100]]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load a dataset (can be replaced with a custom dataset)\n",
    "# dataset = create_dataset(\"Annoted_JSON_090225.json\")\n",
    "\n",
    "# Load a BERT tokenizer\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_fn(batch):\n",
    "    \"\"\"\n",
    "    Tokenizes the batch of tokens and aligns their labels.\n",
    "    \n",
    "    Args:\n",
    "        batch (dict): Batch with keys \"tokens\" and \"labels_numeric\".\n",
    "        \n",
    "    Returns:\n",
    "        dict: Tokenized inputs with aligned labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def align_target(labels, word_ids):\n",
    "        \"\"\"\n",
    "        Aligns the labels with the tokenized word IDs.\n",
    "        \n",
    "        Args:\n",
    "            labels (list): Original labels for a sentence.\n",
    "            word_ids (list): Word IDs from tokenization.\n",
    "        \n",
    "        Returns:\n",
    "            list: Aligned labels (with -100 for special tokens).\n",
    "        \"\"\"\n",
    "        begin2inside = {1: 2, 3: 4, 5: 6, 7: 8}\n",
    "        align_labels = []\n",
    "        last_word = None\n",
    "        for word in word_ids:\n",
    "            if word is None:\n",
    "                label = -100  # For special tokens like [CLS] and [SEP]\n",
    "            elif word != last_word:\n",
    "                label = labels[word]\n",
    "            else:\n",
    "                label = labels[word]\n",
    "            align_labels.append(label)\n",
    "            last_word = word\n",
    "        return align_labels\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "    # Align labels\n",
    "    labels_batch = batch[\"labels_numeric\"]\n",
    "\n",
    "    print(f\"Labels Numeric: {labels_batch}\")\n",
    "\n",
    "    aligned_targets_batch = []\n",
    "    \n",
    "    aligned_targets_batch.append(align_target(labels_batch, tokenized_inputs.word_ids()))\n",
    "\n",
    "    # Add the aligned labels to the tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = aligned_targets_batch\n",
    "    print(f\"Tokenized Labels: {tokenized_inputs}\")\n",
    "    return tokenized_inputs\n",
    "    \n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_fn, batched=True,  remove_columns=dataset['train'].column_names)\n",
    "\n",
    "# Set the dataset format to PyTorch tensors\n",
    "# tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1d0e17-5f14-4918-a517-927caee9cd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets\n",
    "len(dataset['train']['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2794680-be9a-4ac5-a878-6e2404bb49f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6c20c6c-c267-4832-ac40-486fde1f4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names= list(dataset['train']['ner_tags'])\n",
    "label_names= sorted(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907ed3ca-ea28-4d6a-b360-4e379d466126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    ''' This is a simple function which will evaluate the model's output.'''\n",
    "\n",
    "    logits, labels = logits_and_labels \n",
    "\n",
    "     # Load the seqeval metric which can evaluate NER and other sequence tasks\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    # Example usage: compute metric on a sample predictions and reference list \n",
    "    # predictions and references should be a list of lists containing predicted and true token labels\n",
    "    \n",
    "    # List of List Input  \n",
    "    metric.compute(predictions = [['O' , 'B-ORG' , 'I-ORG']], \n",
    "                   references = [['O' , 'B-MISC' , 'I-ORG']])\n",
    "    \n",
    "    # Convert labels from set to list (if needed)\n",
    "    if isinstance(labels, set):\n",
    "        labels = list(labels)\n",
    "\n",
    "    # Get predictions from the logits\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    str_labels = [\n",
    "        [label_names[t] for t in label if t != -100] for label in labels\n",
    "    ]\n",
    "    \n",
    "    str_preds = [\n",
    "        [label_names[p] for (p, t) in zip(prediction, label) if t != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = metric.compute(predictions=str_preds, references=str_labels)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"], \n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]  \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5485b8-f627-4fd3-b632-3dd018cf883a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83310c8787b14c70a801b3ad99c8a801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([31]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([31, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_301/3815360295.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.186843</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.133596</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.096928</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.860927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1662aecc611a459da91a57ab0f1f89a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=5.214365005493164, metrics={'train_runtime': 10.7846, 'train_samples_per_second': 0.278, 'train_steps_per_second': 0.278, 'total_flos': 2786394921984.0, 'train_loss': 5.214365005493164, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  # Location to save fine-tuned model \n",
    "  output_dir = \"fine_tuned_model\",\n",
    "\n",
    "  # Evaluate each epoch\n",
    "  evaluation_strategy = \"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "  # Learning rate for Adam optimizer\n",
    "  learning_rate = 0.001, \n",
    "  \n",
    "  # Batch sizes for training and evaluation\n",
    "  per_device_train_batch_size = 8,\n",
    "  per_device_eval_batch_size = 8,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs = 3,\n",
    "\n",
    "  # L2 weight decay regularization\n",
    "  weight_decay = 0.01,\n",
    "  \n",
    "   no_cuda=True\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer = Trainer(\n",
    "  model=model,  # Use the custom BertCRFNER model\n",
    "  args=training_args,\n",
    "  train_dataset=tokenized_datasets['train'],\n",
    "  eval_dataset=tokenized_datasets['test'],\n",
    "  tokenizer=tokenizer,\n",
    "  compute_metrics=compute_metrics,\n",
    "  data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c50cde-1789-45ca-832c-2d3452d7d794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92979b9-b936-4b6b-adb6-0851dca6fa74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd177f1-3f81-41e7-9ed2-14904d6195ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from new_bert-crf import NERDataset, TokenizerAligner, NERTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eeb2c7-159b-48f0-b032-3227943c9ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d635b-65d6-4e7b-aec2-f72a7a17ba57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac39edab-d00c-4295-a8b9-91996c6fa940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample training data: {'tokens': 'to', 'ner_tags': 'O', 'sentence_id': 1, 'labels_numeric': 13}\n",
      "Training set size: 393\n",
      "Test set size: 99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705efa0194fb4c28b2f023ff3d6bcc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ec326599b0418aa94366992e52f011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and alignment completed.\n",
      "Training model: dbmdz/bert-large-cased-finetuned-conll03-english\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b373bd077da14569997f35c4f3623292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d10b43490b4993b68f7e66b27f4001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and alignment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([31]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([31, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2621/3791639246.py:150: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.664585</td>\n",
       "      <td>0.464052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.720452</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.743522</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: bert-base-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7693cae6cf24a7e87c5dfd16e76a53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c78003440e42869fe4c0847e230791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and alignment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2621/3791639246.py:150: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.533633</td>\n",
       "      <td>0.464052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.541333</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.899964</td>\n",
       "      <td>0.464052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: bert-large-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb8168918a743c8ac0acfd714d92cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2faa99a0884d13a7d1c2c74fd07c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and alignment completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_2621/3791639246.py:150: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>8.859374</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.747200</td>\n",
       "      <td>0.013072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.658297</td>\n",
       "      <td>0.464052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results logged to model_results.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2621/3791639246.py:176: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "# Global label mappings will be set by the dataset loader\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "class NERDataset:\n",
    "    def __init__(self, json_path: str):\n",
    "        self.json_path = json_path\n",
    "        self.dataset = None\n",
    "\n",
    "    def create_dataset(self) -> DatasetDict:\n",
    "        js = pd.read_json(self.json_path, encoding=\"utf-8\")\n",
    "        tokens_flat = []\n",
    "        labels_flat = []\n",
    "        sentence_ids_flat = []\n",
    "\n",
    "        for _, row in js.iterrows():\n",
    "            tokens = row[\"tokens\"]\n",
    "            labels = row[\"labels\"]\n",
    "            sentence_id = row[\"sentence_id\"]  # This is an integer\n",
    "            if len(tokens) == len(labels):\n",
    "                tokens_flat.extend(tokens)\n",
    "                labels_flat.extend(labels)\n",
    "                sentence_ids_flat.extend([sentence_id] * len(tokens))\n",
    "            else:\n",
    "                print(f\"Skipping sentence_id {sentence_id} due to mismatched lengths: {len(tokens)} tokens vs {len(labels)} labels\")\n",
    "\n",
    "        assert len(tokens_flat) == len(labels_flat) == len(sentence_ids_flat), \"Mismatch in list lengths!\"\n",
    "\n",
    "        global id2label, label2id\n",
    "        unique_labels = list(set(labels_flat))\n",
    "        id2label = {idx: label for idx, label in enumerate(unique_labels)}\n",
    "        label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        labels_numeric_flat = [label2id[label] for label in labels_flat]\n",
    "\n",
    "        dataset_dict = {\n",
    "            \"tokens\": tokens_flat,\n",
    "            \"ner_tags\": labels_flat,\n",
    "            \"sentence_id\": sentence_ids_flat,\n",
    "            \"labels_numeric\": labels_numeric_flat\n",
    "        }\n",
    "        dataset = Dataset.from_dict(dataset_dict)\n",
    "        dataset_split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "        self.dataset = DatasetDict({\n",
    "            \"train\": dataset_split[\"train\"],\n",
    "            \"test\": dataset_split[\"test\"]\n",
    "        })\n",
    "\n",
    "        print(\"Sample training data:\", self.dataset[\"train\"][0])\n",
    "        print(f\"Training set size: {len(self.dataset['train'])}\")\n",
    "        print(f\"Test set size: {len(self.dataset['test'])}\")\n",
    "        return self.dataset\n",
    "\n",
    "\n",
    "class TokenizerAligner:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_and_align_labels(self, dataset: DatasetDict) -> DatasetDict:\n",
    "        def tokenize_fn(batch):\n",
    "            def align_target(labels, word_ids):\n",
    "                align_labels = []\n",
    "                for word in word_ids:\n",
    "                    if word is None:\n",
    "                        label = -100  # For special tokens like [CLS] and [SEP]\n",
    "                    else:\n",
    "                        label = labels[word]\n",
    "                    align_labels.append(label)\n",
    "                return align_labels\n",
    "\n",
    "            tokenized_inputs = self.tokenizer(\n",
    "                batch[\"tokens\"],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                is_split_into_words=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"np\"\n",
    "            )\n",
    "            labels_batch = batch[\"labels_numeric\"]\n",
    "            aligned_targets_batch = [align_target(labels_batch, tokenized_inputs.word_ids())]\n",
    "            tokenized_inputs[\"labels\"] = aligned_targets_batch\n",
    "            return tokenized_inputs\n",
    "\n",
    "        tokenized_datasets = dataset.map(tokenize_fn, batched=True, remove_columns=dataset['train'].column_names)\n",
    "        tokenized_datasets.set_format(\"torch\")\n",
    "        print(\"Tokenization and alignment completed.\")\n",
    "        return tokenized_datasets\n",
    "\n",
    "\n",
    "def compute_loss(model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs[\"logits\"]\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "    return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=-1)  # Get the predicted labels\n",
    "    accuracy = (predictions == labels).mean()  # Basic accuracy\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "class NERTrainer:\n",
    "    def __init__(self, model_name: str, tokenizer, tokenized_datasets: DatasetDict):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_datasets = tokenized_datasets\n",
    "        self.data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "        self.device = \"cpu\"  # Change to \"cuda\" if GPU is available\n",
    "\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"fine_tuned_model\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            remove_unused_columns=False,\n",
    "            learning_rate=0.001,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "            no_cuda=True  # Set to False if using a GPU\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.tokenized_datasets['train'],\n",
    "            eval_dataset=self.tokenized_datasets['test'],\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=self.data_collator,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        return trainer\n",
    "\n",
    "\n",
    "def log_results_to_excel(results, excel_path=\"model_results.xlsx\"):\n",
    "    # Check if the Excel file exists\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file doesn't exist, create a new one\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"Accuracy\"])\n",
    "\n",
    "    # Create a new DataFrame from the results to concatenate\n",
    "    new_data = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\"])\n",
    "\n",
    "    # Concatenate the new data with the existing DataFrame\n",
    "    df = pd.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    df.to_excel(excel_path, index=False)\n",
    "    print(f\"Results logged to {excel_path}\")\n",
    "\n",
    "def main():\n",
    "    # Define the list of models\n",
    "    model_names = [\n",
    "        \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "        \"bert-base-cased\",\n",
    "        \"bert-large-uncased\"\n",
    "    ]\n",
    "    \n",
    "    # Path to the JSON file\n",
    "    json_path = \"Annoted_JSON_090225.json\"\n",
    "\n",
    "    # Create dataset\n",
    "    dataset_loader = NERDataset(json_path)\n",
    "    dataset = dataset_loader.create_dataset()\n",
    "\n",
    "    # Tokenize and align labels\n",
    "    tokenizer_aligner = TokenizerAligner(model_names[0])  # Initialize with the first model\n",
    "    tokenized_datasets = tokenizer_aligner.tokenize_and_align_labels(dataset)\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"Training model: {model_name}\")\n",
    "        \n",
    "        # Reinitialize the tokenizer and align the labels for each model\n",
    "        tokenizer_aligner = TokenizerAligner(model_name)\n",
    "        tokenized_datasets = tokenizer_aligner.tokenize_and_align_labels(dataset)\n",
    "        \n",
    "        # Train the model\n",
    "        ner_trainer = NERTrainer(model_name, tokenizer_aligner.tokenizer, tokenized_datasets)\n",
    "        trainer = ner_trainer.train()\n",
    "\n",
    "        # Get the accuracy and store the result\n",
    "        accuracy = trainer.evaluate()[\"eval_accuracy\"]\n",
    "        results.append((model_name, accuracy))\n",
    "\n",
    "    # Log the results to an Excel file\n",
    "    log_results_to_excel(results)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4e87d9-90a1-4f5a-a084-0d22510b04f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: wandb 0.19.8\n",
      "Uninstalling wandb-0.19.8:\n",
      "  Would remove:\n",
      "    /home/jovyan/.local/bin/wandb\n",
      "    /home/jovyan/.local/bin/wb\n",
      "    /home/jovyan/.local/lib/python3.11/site-packages/package_readme.md\n",
      "    /home/jovyan/.local/lib/python3.11/site-packages/wandb-0.19.8.dist-info/*\n",
      "    /home/jovyan/.local/lib/python3.11/site-packages/wandb/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91fe1b9-83d3-46d6-b873-3e5c1f2edac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
